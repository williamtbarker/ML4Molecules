{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/williamtbarker/ML4Molecules/blob/main/exercise_4_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task\n",
        "\n",
        "For this task you will use the QM9 dataset with HOMO as the target value. Perform the following -\n",
        "1. split the dataset with RandomSplitter, ScaffoldSplitter and MolecularWeightSplitterfrom deepchem. You can limit the split to train-test split with 80:20 split. You can use any featurizer.\n",
        "2. for each of the above splits, compare the score on the the test dataset with `MLPPredictor` model for 30 epochs. Does the splitting method affect the model performance?\n",
        "3. for the random split dataset split, change the `hidden_feats` parameter during model creating and train the model again to 30 epochs. Does the R2 score improve with increasing the `hidden_feats`?\n",
        "4. Use the `CircularFingerprint` featurizer and split the dataset randomly. First set the size of the fingerprint to 100. Train the `MLPPredictor` model with 30 epochs. Then change the size of the fingerprints and repeat the training. You can choose the `hidden_feats` our your choice but keep it constant over different fingerprint length. Do the length of fingerprint play a role?"
      ],
      "metadata": {
        "id": "vRqYREyJzA46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepchem dgl dgllife rdkit"
      ],
      "metadata": {
        "id": "_uUHQ32wbz10",
        "outputId": "2798c790-d367-415e-b0f0-e16a3a7aeea5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepchem\n",
            "  Downloading deepchem-2.7.1-py3-none-any.whl (693 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.2/693.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dgl\n",
            "  Downloading dgl-1.1.3-cp310-cp310-manylinux1_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dgllife\n",
            "  Downloading dgllife-0.3.2-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.1/226.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdkit\n",
            "  Downloading rdkit-2023.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.2.2)\n",
            "Collecting scipy<1.9 (from deepchem)\n",
            "  Downloading scipy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.2.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.10/dist-packages (from dgllife) (0.2.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2023.11.17)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deepchem) (3.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from hyperopt->dgllife) (1.16.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt->dgllife) (0.18.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt->dgllife) (2.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt->dgllife) (0.10.9.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2023.3.post1)\n",
            "Installing collected packages: scipy, rdkit, dgl, dgllife, deepchem\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "jax 0.4.23 requires scipy>=1.9, but you have scipy 1.8.1 which is incompatible.\n",
            "jaxlib 0.4.23+cuda12.cudnn89 requires scipy>=1.9, but you have scipy 1.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed deepchem-2.7.1 dgl-1.1.3 dgllife-0.3.2 rdkit-2023.9.4 scipy-1.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember to install all the packages"
      ],
      "metadata": {
        "id": "XMpSlDcJ1d-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import deepchem as dc\n",
        "from rdkit import Chem\n",
        "from dgllife.model.model_zoo.mlp_predictor import MLPPredictor\n",
        "\n",
        "# Load and sample the dataset\n",
        "df = pd.read_csv(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/qm9.csv\")\n",
        "dataset = df[[\"smiles\", \"homo\"]].sample(frac=0.05)\n",
        "\n",
        "# Ensure that the 'smiles' column is a string and check for null values\n",
        "dataset['smiles'] = dataset['smiles'].astype(str)\n",
        "assert dataset['smiles'].apply(lambda x: isinstance(x, str)).all()\n",
        "assert not dataset['smiles'].isnull().any()\n",
        "\n",
        "# Create the featurizer object\n",
        "featurizer = dc.feat.CircularFingerprint(size=100, radius=2)\n",
        "\n",
        "# Featurize the dataset\n",
        "features = [list(featurizer.featurize(smiles)[0]) for smiles in dataset['smiles']]\n",
        "targets = dataset['homo'].values.reshape(-1, 1)\n",
        "\n",
        "# Create a DeepChem dataset\n",
        "dc_dataset = dc.data.NumpyDataset(X=features, y=targets, ids=dataset['smiles'].values)\n",
        "\n",
        "# Function to split the dataset\n",
        "def split_dataset(splitter):\n",
        "    return splitter.train_test_split(dc_dataset, frac_train=0.8)\n",
        "\n",
        "# Split using RandomSplitter\n",
        "random_train, random_test = split_dataset(dc.splits.RandomSplitter())\n",
        "\n",
        "# Split using ScaffoldSplitter\n",
        "# Ensure to use SMILES strings for scaffolding\n",
        "scaffold_splitter = dc.splits.ScaffoldSplitter()\n",
        "scaffold_train, scaffold_test = scaffold_splitter.train_test_split(dc_dataset, frac_train=0.8, smiles_column='ids')\n",
        "\n",
        "# Split using MolecularWeightSplitter\n",
        "mw_train, mw_test = split_dataset(dc.splits.MolecularWeightSplitter())\n",
        "\n",
        "# Now you have train and test sets for each type of splitter\n",
        "\n",
        "from dgllife.model.model_zoo.mlp_predictor import MLPPredictor\n",
        "\n",
        "model = MLPPredictor(in_feats=100, hidden_feats=512, n_tasks=1, dropout=0.)\n",
        "model\n"
      ],
      "metadata": {
        "id": "P1PvkoWSLn1r",
        "outputId": "d61098ab-3f9d-469a-bd78-90a86dda9253",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPPredictor(\n",
              "  (predict): Sequential(\n",
              "    (0): Dropout(p=0.0, inplace=False)\n",
              "    (1): Linear(in_features=100, out_features=512, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): Linear(in_features=512, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from deepchem.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "class TorchDatasetWrapper(Dataset):\n",
        "    \"\"\"A wrapper for the DeepChem NumpyDataset to make it compatible with PyTorch's DataLoader.\"\"\"\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.Tensor(self.dataset.X[idx]), torch.Tensor(self.dataset.y[idx])\n",
        "\n",
        "def train_and_evaluate(train_dataset, test_dataset, epochs=30, batch_size=32, learning_rate=1e-3):\n",
        "    # Wrap the DeepChem datasets\n",
        "    train_dataset = TorchDatasetWrapper(train_dataset)\n",
        "    test_dataset = TorchDatasetWrapper(test_dataset)\n",
        "\n",
        "    # Create DataLoaders for the training and testing datasets\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Define the model\n",
        "    model = MLPPredictor(in_feats=100, hidden_feats=512, n_tasks=1, dropout=0.)\n",
        "\n",
        "    # Move the model to the appropriate device (GPU if available)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            batch_x, batch_y = batch\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            pred = model(batch_x)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(pred, batch_y)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        test_x = torch.Tensor(test_dataset.dataset.X).to(device)\n",
        "        test_y = torch.Tensor(test_dataset.dataset.y).to(device)\n",
        "        y_pred = model(test_x)\n",
        "        mse = mean_squared_error(test_y.cpu().numpy(), y_pred.cpu().numpy())\n",
        "    return mse\n",
        "\n",
        "# Evaluate for each split\n",
        "mse_random = train_and_evaluate(random_train, random_test)\n",
        "print(\"MSE for Random Split:\", mse_random)\n",
        "\n",
        "mse_scaffold = train_and_evaluate(scaffold_train, scaffold_test)\n",
        "print(\"MSE for Scaffold Split:\", mse_scaffold)\n",
        "\n",
        "mse_mw = train_and_evaluate(mw_train, mw_test)\n",
        "print(\"MSE for Molecular Weight Split:\", mse_mw)\n"
      ],
      "metadata": {
        "id": "zpJaMOgIL6ql",
        "outputId": "26dd0273-9ac2-4b16-836a-3af9ffc35a1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Loss: 0.0684589784969354\n",
            "Epoch 2/30, Loss: 0.007510011674769755\n",
            "Epoch 3/30, Loss: 0.003144095797324553\n",
            "Epoch 4/30, Loss: 0.0018610566739447503\n",
            "Epoch 5/30, Loss: 0.0013457789186975874\n",
            "Epoch 6/30, Loss: 0.0010474057423943165\n",
            "Epoch 7/30, Loss: 0.0007836410972924481\n",
            "Epoch 8/30, Loss: 0.0007538598408808909\n",
            "Epoch 9/30, Loss: 0.0007126079321356624\n",
            "Epoch 10/30, Loss: 0.0007513245696567797\n",
            "Epoch 11/30, Loss: 0.0007857285911865931\n",
            "Epoch 12/30, Loss: 0.0007285096267185595\n",
            "Epoch 13/30, Loss: 0.0010221007224962314\n",
            "Epoch 14/30, Loss: 0.0009995445087995557\n",
            "Epoch 15/30, Loss: 0.0011071549940554956\n",
            "Epoch 16/30, Loss: 0.0013534350402464735\n",
            "Epoch 17/30, Loss: 0.0016115713168844757\n",
            "Epoch 18/30, Loss: 0.0013659116604165839\n",
            "Epoch 19/30, Loss: 0.0014163913095087213\n",
            "Epoch 20/30, Loss: 0.0012725840232243562\n",
            "Epoch 21/30, Loss: 0.0015117153934192\n",
            "Epoch 22/30, Loss: 0.0015940574067783366\n",
            "Epoch 23/30, Loss: 0.0016016951915281382\n",
            "Epoch 24/30, Loss: 0.0009531235109967557\n",
            "Epoch 25/30, Loss: 0.0013870710499759298\n",
            "Epoch 26/30, Loss: 0.0010078857064529853\n",
            "Epoch 27/30, Loss: 0.0011538534100899206\n",
            "Epoch 28/30, Loss: 0.00103467453681376\n",
            "Epoch 29/30, Loss: 0.0011888629111732978\n",
            "Epoch 30/30, Loss: 0.0010540770495676302\n",
            "MSE for Random Split: 0.0010527695\n",
            "Epoch 1/30, Loss: 0.06477566321180868\n",
            "Epoch 2/30, Loss: 0.007604800228450802\n",
            "Epoch 3/30, Loss: 0.003381815589992108\n",
            "Epoch 4/30, Loss: 0.0020586091954360847\n",
            "Epoch 5/30, Loss: 0.0015095838500114734\n",
            "Epoch 6/30, Loss: 0.0012476829464763536\n",
            "Epoch 7/30, Loss: 0.0009129905256919475\n",
            "Epoch 8/30, Loss: 0.0008482206517544997\n",
            "Epoch 9/30, Loss: 0.00078285289540266\n",
            "Epoch 10/30, Loss: 0.0007903040873532605\n",
            "Epoch 11/30, Loss: 0.0008901945870353042\n",
            "Epoch 12/30, Loss: 0.0009596287650527388\n",
            "Epoch 13/30, Loss: 0.0009541030943052777\n",
            "Epoch 14/30, Loss: 0.0008111763894530235\n",
            "Epoch 15/30, Loss: 0.0009344382578135646\n",
            "Epoch 16/30, Loss: 0.001001412795414494\n",
            "Epoch 17/30, Loss: 0.0013534852564806073\n",
            "Epoch 18/30, Loss: 0.001249763012830434\n",
            "Epoch 19/30, Loss: 0.0014280595253997792\n",
            "Epoch 20/30, Loss: 0.0012168176557073214\n",
            "Epoch 21/30, Loss: 0.0011283539203023316\n",
            "Epoch 22/30, Loss: 0.0013138278702085483\n",
            "Epoch 23/30, Loss: 0.0014087459591488976\n",
            "Epoch 24/30, Loss: 0.0011323213525645855\n",
            "Epoch 25/30, Loss: 0.001213078415146031\n",
            "Epoch 26/30, Loss: 0.0012886906340662833\n",
            "Epoch 27/30, Loss: 0.0011663743753160816\n",
            "Epoch 28/30, Loss: 0.0009670679220497343\n",
            "Epoch 29/30, Loss: 0.0009560146908846772\n",
            "Epoch 30/30, Loss: 0.0009218510578128709\n",
            "MSE for Scaffold Split: 0.0015324386\n",
            "Epoch 1/30, Loss: 0.0724097905026394\n",
            "Epoch 2/30, Loss: 0.008220110253508514\n",
            "Epoch 3/30, Loss: 0.003428945509118161\n",
            "Epoch 4/30, Loss: 0.0020991844274768873\n",
            "Epoch 5/30, Loss: 0.001497825574049438\n",
            "Epoch 6/30, Loss: 0.001115451667698965\n",
            "Epoch 7/30, Loss: 0.0009628228338745733\n",
            "Epoch 8/30, Loss: 0.0008659978410010115\n",
            "Epoch 9/30, Loss: 0.0008252658831008288\n",
            "Epoch 10/30, Loss: 0.0007913194321666932\n",
            "Epoch 11/30, Loss: 0.0007924953577111453\n",
            "Epoch 12/30, Loss: 0.0008659054349534147\n",
            "Epoch 13/30, Loss: 0.0007749921106941267\n",
            "Epoch 14/30, Loss: 0.0010125308059893239\n",
            "Epoch 15/30, Loss: 0.0010832300845539152\n",
            "Epoch 16/30, Loss: 0.0010263886889290354\n",
            "Epoch 17/30, Loss: 0.0011319820724089542\n",
            "Epoch 18/30, Loss: 0.0012518284721798928\n",
            "Epoch 19/30, Loss: 0.001772587711457163\n",
            "Epoch 20/30, Loss: 0.0016758455700861912\n",
            "Epoch 21/30, Loss: 0.0014480988368935261\n",
            "Epoch 22/30, Loss: 0.0017140242610670005\n",
            "Epoch 23/30, Loss: 0.0014204554290266796\n",
            "Epoch 24/30, Loss: 0.0010098219517246996\n",
            "Epoch 25/30, Loss: 0.0012213951725113606\n",
            "Epoch 26/30, Loss: 0.0010465925157144998\n",
            "Epoch 27/30, Loss: 0.0015263108344654376\n",
            "Epoch 28/30, Loss: 0.0011644546565159025\n",
            "Epoch 29/30, Loss: 0.0011134687764362233\n",
            "Epoch 30/30, Loss: 0.0009317568645721101\n",
            "MSE for Molecular Weight Split: 0.0013107094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can try varying hidden_feats from 64 to 10000. Let the in_feats be constant."
      ],
      "metadata": {
        "id": "FosBqnvl2laR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepchem.metrics import r2_score\n",
        "\n",
        "def train_and_evaluate(train_dataset, test_dataset, hidden_feats, epochs=30, batch_size=32, learning_rate=1e-3):\n",
        "    train_dataset = TorchDatasetWrapper(train_dataset)\n",
        "    test_dataset = TorchDatasetWrapper(test_dataset)\n",
        "\n",
        "    # Define the model with specified hidden_feats\n",
        "    model = MLPPredictor(in_feats=100, hidden_feats=hidden_feats, n_tasks=1, dropout=0.)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            batch_x, batch_y = batch\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            pred = model(batch_x)\n",
        "            loss = criterion(pred, batch_y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_x = torch.Tensor(test_dataset.dataset.X).to(device)\n",
        "        test_y = torch.Tensor(test_dataset.dataset.y).to(device)\n",
        "        y_pred = model(test_x)\n",
        "        r2 = r2_score(test_y.cpu().numpy(), y_pred.cpu().numpy())\n",
        "    return r2\n",
        "\n",
        "# Train and evaluate models with different hidden_feats\n",
        "hidden_feats_list = [64, 128, 256, 512, 1024]\n",
        "for hidden_feats in hidden_feats_list:\n",
        "    r2 = train_and_evaluate(random_train, random_test, hidden_feats)\n",
        "    print(f\"R2 Score with hidden_feats={hidden_feats}: {r2}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "dMqut73CMToH",
        "outputId": "46ae7298-baf4-43e9-aea8-51dac1e19142",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 Score with hidden_feats=64: -0.06385591186452566\n",
            "R2 Score with hidden_feats=128: 0.1971042849978374\n",
            "R2 Score with hidden_feats=256: -0.804712426867253\n",
            "R2 Score with hidden_feats=512: -1.7474898050417695\n",
            "R2 Score with hidden_feats=1024: -3.1500041614723493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can try fingerprint lengths of 32, 64, 128, 512, 1024, 2048, 4096. Have a fixed value for hidden_feats"
      ],
      "metadata": {
        "id": "n0pk2G7t2wWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import deepchem as dc\n",
        "from deepchem.metrics import r2_score\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from dgllife.model.model_zoo.mlp_predictor import MLPPredictor\n",
        "\n",
        "# Load and sample the dataset\n",
        "df = pd.read_csv(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/qm9.csv\")\n",
        "dataset = df[[\"smiles\", \"homo\"]].sample(frac=0.05)\n",
        "\n",
        "class TorchDatasetWrapper(Dataset):\n",
        "    \"\"\"A wrapper for the DeepChem NumpyDataset to make it compatible with PyTorch's DataLoader.\"\"\"\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.Tensor(self.dataset.X[idx]), torch.Tensor(self.dataset.y[idx])\n",
        "\n",
        "def featurize_dataset(smiles, fp_size):\n",
        "    featurizer = dc.feat.CircularFingerprint(size=fp_size, radius=2)\n",
        "    features = [list(featurizer.featurize(s)[0]) for s in smiles]\n",
        "    return features\n",
        "\n",
        "def train_and_evaluate(train_dataset, test_dataset, hidden_feats=128, epochs=30, batch_size=32, learning_rate=1e-3):\n",
        "    train_dataset = TorchDatasetWrapper(train_dataset)\n",
        "    test_dataset = TorchDatasetWrapper(test_dataset)\n",
        "\n",
        "    model = MLPPredictor(in_feats=train_dataset.dataset.X.shape[1], hidden_feats=128, n_tasks=1, dropout=0.)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            batch_x, batch_y = batch\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            pred = model(batch_x)\n",
        "            loss = criterion(pred, batch_y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_x = torch.Tensor(test_dataset.dataset.X).to(device)\n",
        "        test_y = torch.Tensor(test_dataset.dataset.y).to(device)\n",
        "        y_pred = model(test_x)\n",
        "        r2 = r2_score(test_y.cpu().numpy(), y_pred.cpu().numpy())\n",
        "    return r2\n",
        "\n",
        "# Fingerprint lengths to test\n",
        "fp_lengths = [32, 64, 128, 512, 1024, 2048, 4096]\n",
        "\n",
        "for fp_length in fp_lengths:\n",
        "    # Featurize dataset\n",
        "    features = featurize_dataset(dataset['smiles'], fp_length)\n",
        "    targets = dataset['homo'].values.reshape(-1, 1)\n",
        "\n",
        "    # Create DeepChem dataset\n",
        "    dc_dataset = dc.data.NumpyDataset(X=features, y=targets)\n",
        "\n",
        "    # Split the dataset\n",
        "    splitter = dc.splits.RandomSplitter()\n",
        "    train_set, test_set = splitter.train_test_split(dc_dataset, frac_train=0.8)\n",
        "\n",
        "    # Train and evaluate\n",
        "    r2 = train_and_evaluate(train_set, test_set, hidden_feats=128)\n",
        "    print(f\"R2 Score with Fingerprint Length {fp_length}: {r2}\")\n"
      ],
      "metadata": {
        "id": "kIWBgEuQTxx6",
        "outputId": "2b5adf4f-fa97-4b30-f98f-8122daaa60a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 Score with Fingerprint Length 32: -0.3224411234652602\n",
            "R2 Score with Fingerprint Length 64: -1.0378350269848995\n",
            "R2 Score with Fingerprint Length 128: 0.035855875757529465\n",
            "R2 Score with Fingerprint Length 512: 0.24514210326244879\n",
            "R2 Score with Fingerprint Length 1024: 0.41569342695917\n",
            "R2 Score with Fingerprint Length 2048: 0.46243432018985586\n",
            "R2 Score with Fingerprint Length 4096: 0.5413307773015797\n"
          ]
        }
      ]
    }
  ]
}